wget --spider -r -nd -nv -H -l 1 -w 2 -o run1.log  http://your_server_ip/spiderdemo.html

--spider stops wget from downloading the page.
-r makes wget recursively follow each link on the page.
-nd, short for --no-directories, prevents wget from creating a hierarchy of directories on your server (even when it is configured to spider only).
-nv, short for --no-verbose, stops wget from outputting extra information that is unnecessary for identifying broken links.
The following are optional parameters which you can use to customize your search:

-H, short for --span-hosts,makes wget crawl to subdomains and domains other than the primary one (i.e. external sites).
-l 1 is short for --level. By default, wget crawls up to five levels deep from the initial URL, but here we set it to one. You may need to play with this parameter depending on the organization of your website.
-w 2, short for --wait, instructs wget to wait 2 seconds between requests to avoid bombarding the server, minimizing any performance impact.
-o run1.log saves wgetâ€™s output to a file called run1.log instead of displaying it in your terminal.

After you run the above wget command, extract the broken links from the output file using the following command.

grep -B1 'broken link!' run1.log

The -B1 parameter specifies that, for every matching line, wget displays one additional line of leading context before the matching line. 


https://www.deadlinkchecker.com/website-dead-link-checker.asp

https://www.digitalocean.com/community/tutorials/how-to-find-broken-links-on-your-website-using-wget-on-debian-7